{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":96317,"databundleVersionId":11446837,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk\n!pip install wordnet\n# !pip install negspacy\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n!pip install gensim\n!pip install torch-optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:50:58.154180Z","iopub.execute_input":"2025-05-14T17:50:58.154555Z","iopub.status.idle":"2025-05-14T17:51:15.480793Z","shell.execute_reply.started":"2025-05-14T17:50:58.154512Z","shell.execute_reply":"2025-05-14T17:51:15.479781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n!unzip -q glove.twitter*.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:51:15.482211Z","iopub.execute_input":"2025-05-14T17:51:15.482603Z","iopub.status.idle":"2025-05-14T17:56:35.367775Z","shell.execute_reply.started":"2025-05-14T17:51:15.482565Z","shell.execute_reply":"2025-05-14T17:56:35.366369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nimport torch.nn as nn #Base class for all neural network modules.\nimport matplotlib.pyplot as plt\nimport optuna\n# import spacy\nimport gensim\nimport re\nimport random\nimport torch_optimizer as optim\nimport copy\n\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import KeyedVectors\n# from negspacy.negation import Negex\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom collections import Counter\n\n# Set a random seed for reproducibility\nSEED = 42  \n\n# Python's built-in random\nrandom.seed(SEED)\n\n# Numpy\nnp.random.seed(SEED)\n\n# PyTorch\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True \ntorch.backends.cudnn.benchmark = False\n\n# Για PyTorch με CUDA (GPU)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:56:35.369767Z","iopub.execute_input":"2025-05-14T17:56:35.370075Z","iopub.status.idle":"2025-05-14T17:56:56.476766Z","shell.execute_reply.started":"2025-05-14T17:56:35.370049Z","shell.execute_reply":"2025-05-14T17:56:56.476069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/train_dataset.csv')\ntest_data = pd.read_csv('/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/test_dataset.csv')\nval_data = pd.read_csv('/kaggle/input/ai-2-dl-for-nlp-2025-homework-2/val_dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:56:56.478062Z","iopub.execute_input":"2025-05-14T17:56:56.478666Z","iopub.status.idle":"2025-05-14T17:56:57.101659Z","shell.execute_reply.started":"2025-05-14T17:56:56.478628Z","shell.execute_reply":"2025-05-14T17:56:57.100938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# spacy.prefer_gpu()\n# nlp = spacy.load(\"en_core_web_sm\")\n\n# NEGATION_WORDS = {\"no\", \"not\", \"none\", \"neither\", \"nor\", \"never\"}\n# NEGATION_PHRASES = {\n#     \"not at all\": \"not_at_all\",\n#     \"not only\": \"not_only\"\n# }\n# def handle_negation(text):\n#     \"\"\"Επεξεργάζεται την άρνηση στο κείμενο χωρίς να προσθέτει το 'not_' μπροστά από τις λέξεις.\"\"\"\n#     words = word_tokenize(text.lower())  # Χρησιμοποιούμε το word_tokenize για να πάρουμε τις λέξεις\n#     processed_words = []\n#     negated = False  # Εντοπιστής άρνησης\n\n#     for i, word in enumerate(words):\n#         # Ελέγχουμε αν η λέξη είναι 'n't' ή 'no', τα οποία υποστηρίζονται από το GloVe\n#         if word in NEGATION_WORDS:\n#             negated = True\n#             processed_words.append(word)  # Δεν προσθέτουμε το 'not_' γιατί το GloVe το αναγνωρίζει\n#         elif word in NEGATION_PHRASES:  # Ελέγχουμε αν υπάρχει φράση άρνησης\n#             processed_words.append(f\"not_{word.replace(' ', '_')}\")  # Κρατάμε την φράση με 'not_'\n#             negated = False  # Άρνηση τελείωσε\n#         elif negated:\n#             # Αν έχουμε άρνηση και δεν βρούμε 'n't' ή 'no', προσθέτουμε το 'not_' πριν τη λέξη\n#             processed_words.append(f\"not_{word}\")\n#             negated = False  # Τελείωσε η άρνηση\n#         else:\n#             processed_words.append(word)\n\n#     return \" \".join(processed_words)\n\ndef replace_emoticons(text):\n    \"\"\"\n    Replaces common emoticons with text tokens preserving sentiment.\n    Improves accuracy by maintaining emotional context.\n    \"\"\"\n    # Emoticon-to-text mapping dictionary\n    EMOTICON_MAP = {\n        # Positive\n        \":‑)\": \" happy \", \":)\": \" happy \", \":-]\": \" happy \", \n        \":]\": \" happy \", \":-3\": \" happy \", \":3\": \" happy \",\n        \"=]\": \" happy \", \"=)\": \" happy \", \":)\": \" happy \",\n        \"=D\": \" happy\", '<3': ' love ', \n        \n        # Negative\n        \"=(\": \" sad \", \":(\": \" sad \", \":-c\": \" sad \",\n        \":-[\": \" sad \", \":[\": \" sad \", \":{\": \" sad \",\n    }\n    \n    # Replace emoticons\n    for emoticon, replacement in EMOTICON_MAP.items():\n        text = text.replace(emoticon, replacement)\n    \n    # Handle repeated characters (\"Heyyyy\" -> \"Heyy\")\n    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n    \n    # Clean whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndef replace_social_sentiment(text):\n    \"\"\"\n    Replaces social media interjections with corresponding sentiment tokens.\n    Returns original text if no matches found.\n    \"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return text  # Return original for non-text\n    \n    SOCIAL_PATTERNS = {\n        r\"\\bhahaha+\\b\": \"happy\",\n        r\"\\bhehe+\\b\": \"happy\",\n        r\"\\blol\\b\": \"happy\",\n        r\"\\blmao\\b\": \"happy\",\n        r\"\\bbruh+\\b\": \"awkward\",\n        r\"\\boof\\b\": \"awkward\",\n        r\"\\bwow\\b\": \"surprise\",\n        r\"\\bomg\\b\": \"surprise\",\n        r\"\\bugh\\b\": \"annoyed\",\n        r\"\\bsmh\\b\": \"disappointed\"\n    }\n\n    modified = False\n    new_text = text.lower()  # Work in lowercase for consistency\n\n    for pattern, replacement in SOCIAL_PATTERNS.items():\n        if re.search(pattern, new_text):\n            new_text = re.sub(pattern, replacement, new_text)\n            modified = True\n\n    return new_text if modified else text\n\ndef remove_unicode(text):\n    \"\"\"Convert to ASCII and clean special characters\"\"\"\n    text = text.encode('ascii', 'ignore').decode('ascii')\n    # Additional cleaning\n    text = re.sub(r\"[^\\w\\s.,!?;]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndef preprocess_text(text):\n \n    # Unicode normalization\n    text = remove_unicode(text)\n    # Replace emoticons\n    text = replace_emoticons(text)\n    # Replace social expressions\n    text = replace_social_sentiment(text)\n    # Lowercase\n    text = text.lower()\n    # URLs → <url>\n    text = re.sub(r\"http\\S+|www\\.\\S+\", \"<url>\", text)\n    # Mentions → <user>\n    text = re.sub(r\"@\\w+\", \"<user>\", text)\n    # Numbers → <number>\n    text = re.sub(r\"\\d+\", \"<number>\", text)\n    # text = re.sub(r'\\d+', '', text)\n    #Normalize repeated punctuation (!! → !)\n    text = re.sub(r\"([!?.,])\\1{1,}\", r\"\\1\", text)\n    \n    return text\n\n    # text = text.translate(str.maketrans('', '', string.punctuation)) \n    \n    # # Correct the spelling mistakes\n    # text = re.sub(r\"\\b(luv)\\b\", \"love\", text)            \n    # text = re.sub(r\"\\b(amzing)\\b\", \"amazing\", text)\n    # text = re.sub(r\"\\b(terible)\\b\", \"terrible\", text)\n    # text = re.sub(r\"\\b(excelent)\\b\", \"excellent\", text)\n    # text = re.sub(r\"\\b(perfonmence)\\b\", \"performance\", text)\n    # text = re.sub(r\"\\b(gud)\\b\", \"good\", text)\n    # text = re.sub(r\"\\b(vry)\\b\", \"very\", text)\n    # text = re.sub(r\"\\b(fanstic)\\b\", \"fantastic\", text)\n    # text = re.sub(r\"\\b(gr8)\\b\", \"great\", text)\n    # text = re.sub(r\"\\b(horrble)\\b\", \"horrible\", text)\n    # text = re.sub(r\"\\b(u)\\b\", \"you\", text)\n    # text = re.sub(r\"\\b(guyz)\\b\", \"guys\", text)\n    # text = re.sub(r\"\\b(knw)\\b\", \"know\", text)\n    # text = re.sub(r\"\\b(da)\\b\", \"the\", text)\n    # text = re.sub(r\"\\b(btw)\\b\", \"by the way\", text)\n    # text = re.sub(r\"\\b(r)\\b\", \"are\", text)\n    # text = re.sub(r\"\\b(cuz)\\b\", \"because\", text)\n    # text = re.sub(r\"\\b(tho)\\b\", \"though\", text)\n    # text = re.sub(r\"\\b(lol)\\b\", \"laugh out loud\", text)\n    # text = re.sub(r\"\\b(ur)\\b\", \"your\", text)\n\n\n    \n    # tokens = tweet_tokenizer.tokenize(text)  # Tokenization using TweetTokenizer\n\n    # return \" \".join(tokens)  # Return processed text as string\n\n    # #Tokenization\n    # words = word_tokenize(text)\n    # return \" \".join(words)\n\n    # Remove custom stopwords or \n    # Remove stopwords but keep negations\n    # words = [word for word in words if word not in stop_words]\n    # processed_text = ' '.join(words)\n    \n    # words = [word for word in words if word not in custom_stopwords]\n\n    # Lemmatization \n    # lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n    # processed_text = ' '.join(lemmatized_words)\n    # return processed_text\n\n    # tagged_words = pos_tag(words)\n    # # Keep only nouns (NN) and verbs (VB) //no\n    # filtered_words = [word for word, tag in tagged_words if tag.startswith('NN') or tag.startswith('VB')]\n    # # Rejoin words into text\n    # processed_text = ' '.join(filtered_words)\n    # return processed_text\n\n    # # Stemming\n    # stemmed_words = [stemmer.stem(word) for word in words] #probably not\n    # # Join words back into a single string\n    # processed_text = ' '.join(stemmed_words)\n    # return processed_text\n\n\n# def correct_spelling(text):\n#     blob = TextBlob(text)\n#     corrected_text = str(blob.correct())\n#     return corrected_text\n\n\n# Spelling correction\n# train_data['Text'] = train_data['Text'].apply(correct_spelling)\n# test_data['Text'] = test_data['Text'].apply(correct_spelling)\n# val_data['Text'] = val_data['Text'].apply(correct_spelling)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:56:57.102517Z","iopub.execute_input":"2025-05-14T17:56:57.102845Z","iopub.status.idle":"2025-05-14T17:56:57.112770Z","shell.execute_reply.started":"2025-05-14T17:56:57.102821Z","shell.execute_reply":"2025-05-14T17:56:57.111920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data['clean_text'] = train_data['Text'].apply(preprocess_text)\nval_data['clean_text'] = val_data['Text'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:56:57.113616Z","iopub.execute_input":"2025-05-14T17:56:57.113946Z","iopub.status.idle":"2025-05-14T17:57:07.280135Z","shell.execute_reply.started":"2025-05-14T17:56:57.113919Z","shell.execute_reply":"2025-05-14T17:57:07.279502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a function to find most frequent out-of-vocabulary (OOV) words\ndef get_oov_words(texts, glove_vocab, top_k=50):\n    \"\"\"Identify top K most frequent words not in GloVe vocabulary\"\"\"\n    all_tokens = []\n    # Tokenize all texts and collect tokens\n    for text in texts:\n        tokens = preprocess_text(text).split()\n        all_tokens.extend(tokens)\n    \n    # Count word frequencies\n    counter = Counter(all_tokens)\n    # Filter words not in GloVe vocabulary\n    oov_words = {word: freq for word, freq in counter.items() if word not in glove_vocab}\n    # Sort by frequency (descending)\n    sorted_oov = sorted(oov_words.items(), key=lambda x: -x[1])\n    return sorted_oov[:top_k]  # Return top K most frequent OOV words\n\n# Load GloVe vocabulary (one-time operation)\nglove_path = \"glove.twitter.27B.200d.txt\"  # Update path as needed\n\nglove_vocab = set()  # Store unique words from GloVe\nwith open(glove_path, \"r\", encoding=\"utf8\") as f:\n    for line in f:\n        word = line.split(\" \")[0]  # First element is the word\n        glove_vocab.add(word)  # Add to vocabulary set\n\n# Find top OOV words in training and validation sets\noov_train = get_oov_words(train_data['clean_text'], glove_vocab)\noov_val = get_oov_words(val_data['clean_text'], glove_vocab)\n\n# Print results\nprint(\"Top OOV words in train:\")\nfor word, freq in oov_train:\n    print(f\"{word}: {freq}\")  # word: frequency count\n\nprint(\"\\nTop OOV words in val:\")\nfor word, freq in oov_val:\n    print(f\"{word}: {freq}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:57:07.280861Z","iopub.execute_input":"2025-05-14T17:57:07.281071Z","iopub.status.idle":"2025-05-14T17:57:33.337078Z","shell.execute_reply.started":"2025-05-14T17:57:07.281053Z","shell.execute_reply":"2025-05-14T17:57:33.336226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# word2vec_model = api.load(\"word2vec-google-news-300\")\n# glove_input_file = 'glove.6B.200d.txt'\nglove_input_file = \"glove.twitter.27B.200d.txt\"\n\nmodel = KeyedVectors.load_word2vec_format(glove_input_file, binary=False, no_header=True)\n\n\n# # Δείγμα 20 πρώτων λέξεων\n# w2v_dict = {\n#     'word': [],\n#     'vector': []\n# }\n\n# for i, word in enumerate(model.index_to_key[:50]):\n#     w2v_dict['word'].append(word)\n#     w2v_dict['vector'].append(model[word])\n\n\n# w2v_df = pd.DataFrame.from_dict(w2v_dict)\n# w2v_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:57:33.339038Z","iopub.execute_input":"2025-05-14T17:57:33.339259Z","iopub.status.idle":"2025-05-14T17:59:46.167995Z","shell.execute_reply.started":"2025-05-14T17:57:33.339241Z","shell.execute_reply":"2025-05-14T17:59:46.167282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize tweet tokenizer for splitting text into tokens\ntweet_tokenizer = TweetTokenizer()\n\ndef text_to_embedding(text, embedding_model=model, embedding_dim=200):\n    \"\"\"Convert text to embedding by averaging token vectors\"\"\"\n    \n    # Tokenize input text\n    tokens = tweet_tokenizer.tokenize(text)\n    \n    # Get vectors for each token\n    vectors = []\n    for token in tokens:\n        if token in embedding_model:  # Check if token exists in embedding model\n            vectors.append(embedding_model[token])  # Use pre-trained embedding\n        else:\n            vectors.append(np.zeros(embedding_dim))  # Zero vector for out-of-vocabulary tokens\n    \n    # Return average of all token vectors\n    if vectors:\n        return np.mean(vectors, axis=0)  # Average pooling\n    else:\n        return np.zeros(embedding_dim)  # Fallback for empty text\n\n# Apply embedding function to datasets\ntrain_data['vector'] = train_data['clean_text'].apply(text_to_embedding)  # Embed training texts\nval_data['vector'] = val_data['clean_text'].apply(text_to_embedding)  # Embed validation texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T17:59:46.169110Z","iopub.execute_input":"2025-05-14T17:59:46.169326Z","iopub.status.idle":"2025-05-14T18:00:03.236850Z","shell.execute_reply.started":"2025-05-14T17:59:46.169305Z","shell.execute_reply":"2025-05-14T18:00:03.235947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Convert embeddings to PyTorch tensors\n# For training set\nx_train = torch.tensor(np.stack(train_data['vector'].values), dtype=torch.float32)  # Stack embeddings into matrix\ny_train = torch.tensor(train_data['Label'].values, dtype=torch.float32)  # Convert labels to tensor\n\n# For validation set\nx_val = torch.tensor(np.stack(val_data['vector'].values), dtype=torch.float32)\ny_val = torch.tensor(val_data['Label'].values, dtype=torch.float32)\n\n# Create PyTorch Dataset objects\ntrain_dataset = TensorDataset(x_train, y_train)  # (features, labels) pairs\nval_dataset = TensorDataset(x_val, y_val)\n\n# Print tensor shapes for verification\nprint(f\"x_train shape: {x_train.shape}\")  # [num_samples, embedding_dimension]\nprint(f\"y_train shape: {y_train.shape}\")  # [num_samples]\nprint(f\"x_val shape: {x_val.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:00:03.237727Z","iopub.execute_input":"2025-05-14T18:00:03.238009Z","iopub.status.idle":"2025-05-14T18:00:03.660656Z","shell.execute_reply.started":"2025-05-14T18:00:03.237974Z","shell.execute_reply":"2025-05-14T18:00:03.659659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DynamicNet(nn.Module):\n    def __init__(self, D_in, H1, H2, D_out=1, num_layers=2, dropout=0.3, batch_norm=False):\n        super(DynamicNet, self).__init__()\n        self.layers = nn.ModuleList()  # Flexible layer container\n\n        # First layer (input to first hidden)\n        self.layers.append(nn.Linear(D_in, H1))\n        if batch_norm:\n            self.layers.append(nn.BatchNorm1d(H1))  # Normalization\n        self.layers.append(nn.GELU())  # Activation\n        self.layers.append(nn.Dropout(dropout))  # Regularization\n\n        # Additional hidden layers\n        current_dim = H1\n        for _ in range(num_layers - 1):\n            self.layers.append(nn.Linear(current_dim, H2))\n            if batch_norm:\n                self.layers.append(nn.BatchNorm1d(H2))\n            self.layers.append(nn.GELU())\n            self.layers.append(nn.Dropout(dropout))\n            current_dim = H2  # Update dimension for next layer\n\n        # Final output layer (no activation - handled by loss)\n        self.layers.append(nn.Linear(current_dim, D_out))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)  # Sequential processing\n        return x  # Raw logits for BCEWithLogitsLoss\n\n# Model configuration (tuned via Optuna)\nD_in = x_train.shape[1]  # Input dimension from data\nH1 = 344  # First hidden layer size (optimized)\nH2 = 224  # Second hidden layer size (optimized)\nD_out = 1  # Binary classification output\nnum_layers = 2  # Total hidden layers\ndropout = 0.3  # Dropout rate (found optimal)\nuse_batch_norm = True  # Batch normalization flag\n\n# Training parameters (optimized)\nlearning_rate = 0.00012538928471634227  # LR from Optuna\nweight_d = 0.000  # L2 regularization (disabled)\nbatch_size = 128  # Batch size (optimized)\n\n# Initialize model with best parameters\nmodel = DynamicNet(D_in, H1, H2, D_out, num_layers, dropout, use_batch_norm)\n\n# Loss function (combines sigmoid + BCE)\nloss_func = nn.BCEWithLogitsLoss()\n\n# Adam optimizer with tuned learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_d)\n\n# LR scheduler (reduces LR on plateau)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n\n# Prepare datasets and loaders\ntrain_dataset = TensorDataset(x_train, y_train)\nval_dataset = TensorDataset(x_val, y_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:00:03.661612Z","iopub.execute_input":"2025-05-14T18:00:03.661893Z","iopub.status.idle":"2025-05-14T18:00:06.056473Z","shell.execute_reply.started":"2025-05-14T18:00:03.661870Z","shell.execute_reply":"2025-05-14T18:00:06.055779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize tracking variables\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nbest_val_loss = float('inf')  # Track best validation loss\npatience_counter = 0\nbest_model_state = None\npatience = 5  # Early stopping patience\nepochs = 30  # Max epochs\n\n# Training phase\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    all_train_preds = []\n    all_train_labels = []\n\n    # Forward pass + backprop\n    for x_batch, y_batch in train_loader:\n        y_batch = y_batch.view(-1, 1).float()\n        optimizer.zero_grad()\n        logits = model(x_batch)\n        loss = loss_func(logits, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        all_train_preds.append((torch.sigmoid(logits) >= 0.5).int())\n        all_train_labels.append(y_batch.int())\n\n    # Calculate epoch training metrics\n    avg_train_loss = train_loss / len(train_loader)\n    train_preds = torch.cat(all_train_preds).squeeze()\n    train_labels = torch.cat(all_train_labels).squeeze()\n    train_accuracy = accuracy_score(train_labels.cpu(), train_preds.cpu())\n    \n # VALIDATION\n    model.eval()\n    val_loss = 0.0\n    all_val_preds = []\n    all_val_labels = []\n\n    with torch.no_grad():\n        for x_val_batch, y_val_batch in val_loader:\n            y_val_batch = y_val_batch.view(-1, 1).float()\n            val_logits = model(x_val_batch)\n            loss = loss_func(val_logits, y_val_batch)\n            val_loss += loss.item()\n            all_val_preds.append((torch.sigmoid(val_logits) >= 0.5).int())\n            all_val_labels.append(y_val_batch.int())\n\n    # Calculate epoch validation metrics \n    avg_val_loss = val_loss / len(val_loader)\n    val_preds = torch.cat(all_val_preds).squeeze()\n    val_labels = torch.cat(all_val_labels).squeeze()\n    val_accuracy = accuracy_score(val_labels.cpu(), val_preds.cpu())\n\n    train_losses.append(avg_train_loss)\n    val_losses.append(avg_val_loss)\n    train_accuracies.append(train_accuracy)\n    val_accuracies.append(val_accuracy)\n\n    # Store metrics\n    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n          f\"Train Acc: {train_accuracy:.4f} | Val Acc: {val_accuracy:.4f}\")\n\n    # Update learning rate scheduler\n    scheduler.step(avg_val_loss)\n\n     # Early stopping check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = copy.deepcopy(model.state_dict())\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n        if patience_counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# Restore best model\nif best_model_state:\n    model.load_state_dict(best_model_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:00:06.057201Z","iopub.execute_input":"2025-05-14T18:00:06.057702Z","iopub.status.idle":"2025-05-14T18:02:27.506366Z","shell.execute_reply.started":"2025-05-14T18:00:06.057664Z","shell.execute_reply":"2025-05-14T18:02:27.505358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    logits = model(x_val)\n    probs = torch.sigmoid(logits)  # Convert logits to probabilities\n    y_val_pred = (probs >= 0.5).int().squeeze()  # Convert to binary predictions (0/1)\n\n# Print validation metrics\nprint(\"Validation Metrics:\")\nprint(\"------------------\")\nprint(f\"Accuracy:  {accuracy_score(y_val, y_val_pred):.6f}\")  # Classification accuracy\nprint(f\"Precision: {precision_score(y_val, y_val_pred):.6f}\")  # Positive predictive value\nprint(f\"Recall:    {recall_score(y_val, y_val_pred):.6f}\")     # True positive rate\nprint(f\"F1 Score:  {f1_score(y_val, y_val_pred):.6f}\")         # Harmonic mean of precision/recall\nprint()\n\n# Training set evaluation\nwith torch.no_grad():\n    logits = model(x_train)\n    probs = torch.sigmoid(logits)\n    y_train_pred = (probs >= 0.5).int().squeeze()\n\nprint(\"Training Metrics:\")\nprint(\"------------------\")\nprint(f\"Accuracy:  {accuracy_score(y_train, y_train_pred):.6f}\")  # Model performance on training data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:02:27.507542Z","iopub.execute_input":"2025-05-14T18:02:27.507893Z","iopub.status.idle":"2025-05-14T18:02:28.684453Z","shell.execute_reply.started":"2025-05-14T18:02:27.507855Z","shell.execute_reply":"2025-05-14T18:02:28.683597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\n\n# Define data and x-axis range based on training duration\nepochs_range = range(1, len(train_losses) + 1)  # Automatically covers all epochs (e.g., 25)\n\n# Plot loss curves (blue and green)\nplt.plot(epochs_range, train_losses, label='Training Loss', color='blue', linestyle='-', linewidth=2)\nplt.plot(epochs_range, val_losses, label='Validation Loss', color='green', linestyle='-', linewidth=2)\n\n# Plot accuracy curves (red and orange)\nplt.plot(epochs_range, train_accuracies, label='Training Accuracy', color='red', linestyle='-', linewidth=2)\nplt.plot(epochs_range, val_accuracies, label='Validation Accuracy', color='orange', linestyle='-', linewidth=2)\n\n# Titles and axes\nplt.title('Learning Curves', fontsize=14, pad=20)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Metric Value', fontsize=12)\nplt.xticks(epochs_range[::3])  # Show every 3rd epoch \nplt.yticks(np.arange(0.2, 1.1, 0.1))  # Y-axis ticks from 0.2 to 1.0 in 0.1 steps\nplt.ylim(0.2, 1.0)  # Fixed y-range for accuracy (adjust if loss exceeds 1.0)\n\n# Legend and grid\nplt.legend(loc='upper right', framealpha=1.0)\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Display\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:02:28.685595Z","iopub.execute_input":"2025-05-14T18:02:28.685939Z","iopub.status.idle":"2025-05-14T18:02:29.026444Z","shell.execute_reply.started":"2025-05-14T18:02:28.685905Z","shell.execute_reply":"2025-05-14T18:02:29.025405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n\n# Calculate and plot ROC curve\nval_probs = []  # Model's predicted probabilities\nval_true_labels = []  # Actual labels\n\nwith torch.no_grad():\n    for x_val_batch, y_val_batch in val_loader:\n        logits = model(x_val_batch)\n        val_probs.extend(torch.sigmoid(logits).cpu().numpy())\n        val_true_labels.extend(y_val_batch.float().cpu().numpy())\n\n# Compute ROC metrics\nfpr, tpr, _ = roc_curve(val_true_labels, val_probs)\nroc_auc = auc(fpr, tpr)\n\n# Plot configuration\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f'AUC={roc_auc:.2f}')  # Model's performance\nplt.plot([0,1],[0,1], 'k--')  # Random baseline\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:02:29.027412Z","iopub.execute_input":"2025-05-14T18:02:29.027732Z","iopub.status.idle":"2025-05-14T18:02:29.850205Z","shell.execute_reply.started":"2025-05-14T18:02:29.027704Z","shell.execute_reply":"2025-05-14T18:02:29.849446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def suggest_hyperparameters(trial):\n#     H1 = trial.suggest_int(\"H1\", 120, 376, step=32)\n#     H2 = trial.suggest_int(\"H2\", 64, 256, step=32)\n#     num_layers = trial.suggest_int(\"num_layers\", 2, 3)\n\n#     # Create parameter dictionary with all tunable hyperparameters\n#     params = {\n#         \"H1\": H1, \"H2\": H2, \"num_layers\": num_layers,\n#         \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\"]),\n#         \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True),\n#         \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128]),\n#         \"weight_d\": trial.suggest_categorical(\"weight_d\", [0.0, 1e-6, 1e-5, 1e-4]),\n#         \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.1),\n#         \"batch_norm\": trial.suggest_categorical(\"batch_norm\", [True])\n#     }\n\n#     return params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:02:29.850870Z","iopub.execute_input":"2025-05-14T18:02:29.851072Z","iopub.status.idle":"2025-05-14T18:02:29.854385Z","shell.execute_reply.started":"2025-05-14T18:02:29.851054Z","shell.execute_reply":"2025-05-14T18:02:29.853673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Set device to GPU if available, otherwise CPU\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# def objective(trial):\n#     # Get hyperparameters from Optuna trial\n#     params = suggest_hyperparameters(trial)\n    \n#     # Create data loaders with current batch size\n#     train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n#     val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"])\n    \n#     # Initialize model with dynamic architecture based on trial parameters\n#     D_in = x_train.shape[1]  # Input dimension from training data\n#     model = DynamicNet(\n#         D_in=D_in, D_out=1,  # Binary classification output\n#         H1=params[\"H1\"], H2=params[\"H2\"],  # Hidden layer sizes\n#         num_layers=params[\"num_layers\"],  # Number of hidden layers\n#         dropout=params[\"dropout\"],  # Dropout rate\n#         batch_norm=params[\"batch_norm\"]  # Batch normalization flag\n#     ).to(device)  # Move model to GPU if available\n    \n#     # Configure optimizer based on trial suggestion\n#     optimizer_classes = {\n#         \"Adam\": torch.optim.Adam,\n#         \"AdamW\": torch.optim.AdamW\n#     }\n#     optimizer = optimizer_classes[params[\"optimizer\"]](\n#         model.parameters(), \n#         lr=params[\"lr\"],  # Learning rate\n#         weight_decay=params[\"weight_d\"]  # L2 regularization\n#     )\n    \n#     # Loss function and learning rate scheduler\n#     loss_func = nn.BCEWithLogitsLoss()  # Binary cross-entropy with logits\n#     scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n    \n#     # Early stopping configuration\n#     patience = 5\n#     best_val_loss = float('inf')\n#     patience_counter = 0\n#     best_model_state = None\n#     max_epochs = 30\n    \n#     # Training loop\n#     for epoch in range(max_epochs):\n#         model.train()\n#         train_loss = 0.0\n        \n#         # Batch training\n#         for x_batch, y_batch in train_loader:\n#             x_batch = x_batch.to(device)\n#             y_batch = y_batch.view(-1, 1).float().to(device)\n            \n#             optimizer.zero_grad()\n#             logits = model(x_batch)\n#             loss = loss_func(logits, y_batch)\n#             loss.backward()\n#             optimizer.step()\n#             train_loss += loss.item()\n        \n#         # Validation phase\n#         model.eval()\n#         val_loss = 0.0\n#         with torch.no_grad():\n#             for x_val_batch, y_val_batch in val_loader:\n#                 x_val_batch = x_val_batch.to(device)\n#                 y_val_batch = y_val_batch.view(-1, 1).float().to(device)\n                \n#                 val_logits = model(x_val_batch)\n#                 loss = loss_func(val_logits, y_val_batch)\n#                 val_loss += loss.item()\n        \n#         avg_val_loss = val_loss / len(val_loader)\n#         scheduler.step(avg_val_loss)  # Adjust learning rate\n        \n#         # Early stopping check\n#         if avg_val_loss < best_val_loss:\n#             best_val_loss = avg_val_loss\n#             patience_counter = 0\n#             best_model_state = copy.deepcopy(model.state_dict())\n#         else:\n#             patience_counter += 1\n#             if patience_counter >= patience:\n#                 break\n    \n#     # Load best model state before evaluation\n#     if best_model_state:\n#         model.load_state_dict(best_model_state)\n    \n#     # Final evaluation on validation set\n#     model.eval()\n#     with torch.no_grad():\n#         x_val_gpu = x_val.to(device)\n#         logits = model(x_val_gpu)\n#         probs = torch.sigmoid(logits)\n#         y_pred = (probs >= 0.5).int().cpu().squeeze()\n    \n#     acc = accuracy_score(y_val.int(), y_pred)\n#     return acc  # Return accuracy for Optuna optimization\n\n# # Configure Optuna study\n# study = optuna.create_study(\n#     direction=\"maximize\",  # We want to maximize accuracy\n#     study_name=\"advanced_nn_tuning\",\n#     sampler=optuna.samplers.TPESampler(),  # Tree-structured Parzen Estimator\n#     pruner=optuna.pruners.HyperbandPruner()  # Early pruning of unpromising trials\n# )\n\n# # Run optimization\n# study.optimize(objective, n_trials=20, timeout=3600)  # 20 trials or 1 hour max\n\n# # Print results\n# print(\"Best Params:\", study.best_trial.params)\n# print(\"Best Accuracy:\", study.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:02:29.855256Z","iopub.execute_input":"2025-05-14T18:02:29.855535Z","iopub.status.idle":"2025-05-14T18:02:29.870216Z","shell.execute_reply.started":"2025-05-14T18:02:29.855515Z","shell.execute_reply":"2025-05-14T18:02:29.869492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data['clean_text'] = test_data['Text'].apply(preprocess_text)\ntest_data['vector'] = test_data['clean_text'].apply(text_to_embedding)\n\nx_test =  torch.tensor(np.stack(test_data['vector'].values), dtype=torch.float32)\nmodel.eval()  \nwith torch.no_grad():\n    logits = model(x_test)\n    probs = torch.sigmoid(logits)\n    test_predictions = (probs >= 0.5).int().squeeze()\n\nsubmission = pd.DataFrame({\n    'ID': test_data['ID'],  \n    'Label': test_predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully!\")\nprint(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T18:02:29.870877Z","iopub.execute_input":"2025-05-14T18:02:29.871050Z","iopub.status.idle":"2025-05-14T18:02:33.090105Z","shell.execute_reply.started":"2025-05-14T18:02:29.871034Z","shell.execute_reply":"2025-05-14T18:02:33.089286Z"}},"outputs":[],"execution_count":null}]}